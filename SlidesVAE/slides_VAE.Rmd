---
title: "A gentle introduction to the Variational Neural Networks"
author: "J. Aubert and S. Donnet for StateOfTheR"
date: "Dec. 2021"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Context

-   In statistical learning, two main tasks:

    -   **Regression or classification**
    -   **Reduction of dimension**

-   Neural networks are used to construct the regression function, classifier or encoder-decoder (**autoencoder**).

-   **Variational versions** are used when we do not want to optimize a parameter but a **probability distribution**

---
# Organisation  of the talk

-   Relies on

1. **Regression and classification, reduction of dimension**

2. **Neural networks**

3. **Variational neural networks**

4. **A few reminder on the optimization procedure**


---
class: center, middle

1. **Regression and classification, reduction of dimension**

---

# Regression or classification

<br>

-   Let $(\mathbf{X},\mathbf{Y})$ be our dataset:

-   $(\mathbf{X},\mathbf{Y})=(X_i, Y_i)_{i \in 1, \dots,N_{obs}}$

-   $\forall i =1,\dots,N_{obs}$, **Variables** $X_i \in \mathbb{R}^n$.\

-   $Y_i \in \mathcal{Y}$ the variable to explain : **classification** or **regression**

-   Looking for a function $f$

    -   **classifier** or **regression** $f :\mathbb{R}^n \mapsto \mathcal{Y}$ and

    -   such that $$Y \approx f(X) \Leftrightarrow \mbox{Loss}(Y -   f(X)) \mbox{ small } $$

        -   If **regression** $\mbox{Loss}(Y - f(X)) = ||Y - f(X)) ||^2$

        -   

            ## If **classification** : Loss = cross-entropy

# Regression or classification

```{r, echo=FALSE, out.width="90%", fig.cap="Regression or classification",fig.align="center"}
knitr::include_graphics("images/RegressionClassification.png")
```

------------------------------------------------------------------------

# Reduction of dimension

**Auto-encoders** are used for the reduction of dimension of (large) datasets.

<br> - Let $X$ be our dataset: $\mathbf{X}=(X_i)_{i \in 1, \dots,N_{obs}}$

-   $\forall i =1,\dots,N_{obs}$, $X_i \in \mathbb{R}^n$.

-   Looking for two functions

    -   **Encoder** $e :\mathbb{R}^n \mapsto \mathbb{R}^m$ and

    -   **Decoder** $d :\mathbb{R}^m \mapsto \mathbb{R}^n$

    -   such that $$X \approx d(e(X)) \Leftrightarrow ||X -   d(e(X)) ||^2 \mbox{ small } $$

    -   $Z = e(X)$ : **latent variable**

------------------------------------------------------------------------

# Autoencoder

```{r, echo=FALSE, out.width="90%", fig.cap="Autoencoder scheme.",fig.align="center"}
knitr::include_graphics("images/Autoencoder.png")
```

---
class:   middle

**Neural networks**



---

# About $f$: neural networks

```{r, echo=FALSE, out.width="90%", fig.cap="Neural network architecture.",fig.align="center"}
knitr::include_graphics("images/NeuralNetwork.png")
```

------------------------------------------------------------------------

# About $d$ and $e$ : neural networks

```{r, echo=FALSE, out.width="90%", fig.cap="Autoencoder scheme.",fig.align="center"}
knitr::include_graphics("images/Autoencoder2.png")
```

------------------------------------------------------------------------

# About neural networks

**One neuron** : $f_j (\mathbf{x}) = \phi (<w_j, \mathbf {x}> + \, b_j)$ where

-   $\phi$ the activation function : non linear

-   The quantities $w_j = (w_j^1, \dots, w_j^n)$ are the weights of the input variables $(x^1, \dots, x^n)$

-   $b_j$ is the bias of neuron $j$.

**At each layer** $\ell$ of the neural network:

-   Receive $n_{\ell-1}$ input variables $\mathbf{y}^{\ell-1} =(y^{\ell-1}_{1}, \dots,y^{\ell-1}_{n_{\ell-1}})$

-   Create $n_\ell$ new variables. For variable $j$ of layer $l$: $$y^{\ell}_{j} = \phi(<w^\ell_j, \mathbf{y}^{\ell-1}>  +  b^{\ell}_j)$$

**Unknown parameters** $\theta$\
- $w^\ell_j \in \mathbb{R}^{n_\ell-1}$, for $\ell =1, \dots L$, for $j=1,\dots,n_{\ell}$, - $b^\ell_j \in \mathbb{R}$, for $\ell =1, \dots L$, for $j=1,\dots,n_{\ell}$,

------------------------------------------------------------------------

# Model choice

**To choose**:

-   The number of layers $L$

-   The number of neurons in each layer: $n_\ell$ :

    -   possibly $n_\ell > n$
    -   For autoencoder the middle layer $m < n$

-   The activation function $\phi$

------------------------------------------------------------------------

# Learning $f,d$ and $e$

<br>

-   **Regression or classification**

The parameters $\theta = (w^\ell_j,b^\ell_j)_{j = 1\dots,n_\ell, \ell = 1,\dots,L}$ are calibrated on a dataset $(X_i,Y_i)_{i=1, \dots , N_{obs}}$ by minimizing the loss function

$$\widehat{\theta} = \mbox{argmin}_{\theta \in\Theta}  \sum_{i=1}^{N_{obs}}\mbox{Loss}(Y_i - f_{\theta}(X_i))$$

-   **Autoencoder**

The parameters $\theta = (w^\ell_j,b^\ell_j)_{j = 1\dots,n_\ell, \ell = 1,\dots,L}$ are calibrated on a dataset $(X_i)_{i=1, \dots , N_{obs}}$ by minimizing the loss function

$$\widehat{\theta} = \mbox{argmin}_{\theta \in\Theta}  \sum_{i=1}^{N_{obs}}||X_i - d_{\theta}\circ e_{\theta}(X_i)||^2$$

**Optimisation by Stochastic gradient descent**: see later for a reminder of the principle

------------------------------------------------------------------------

# PCA versus autoencoder

-   Let $W \in M_{n,m}(\mathbb{R})$, $W = (C_1,\dots,C_m)$

-   The $C_i$'s are $m$ columns vectors, $m < n$.

-   **Hyp.**: $(C_i)$ are orthonormal vectors. Consequently: $$W'W = I_n$$

-   Let $W' X_i$ is the projector of vector $X_i$ on the sub-vectorial space generated by the columns of $W$.

-   We are looking for $W$ minimizing the inertia of the projected dataset: $$
    \begin{aligned}
    W^* &=\mbox{argmax}_{\{W \in M_{n,m}(\mathbb{R}), W'W = I_n\}} \sum_{i=1}^{N_{obs}} || W'X_i||^2\\ &=\mbox{argmin}_{\{W \in M_{n,m}(\mathbb{R}), W'W = I_n\}} \sum_{i=1}^{N_{obs}} || X_i - WW'X_i||^2
    \end{aligned}
    $$

------------------------------------------------------------------------

# PCA versus autoencoder

-   $W' = e$ **linear** encoder function

-   $W = d$ : **linear** decoder function

-   Note that if you use neural networks with linear activation function and one layer, you will get $W$ not necessarily orthogonal.

[Lien vers une dÃ©monstration propre](http://www.xavierdupre.fr/app/mlstatpy/helpsphinx/c_ml/rn/rn_9_auto.html)

---
class: middle

**Variational versions**


---

    -   Prior on $\theta$: $\pi(\theta)$

    -   Estimation not of $\theta$ but of the posterior distribution of $\theta$ : $p(\theta | \mathbf{Y})$

-   **Autoencoder**: give a structure on the latent space $\mathbf{Z}$

    -   Prior on $Z$: $\pi(Z)$

    -   Estimation of $\theta$ and of the posterior distribution of $Z$ : $p(Z | \theta, \mathbf{X})$

-   **Variational** : approximation of the distributions

    -   $p(\theta | \mathbf{Y}) \approx q_\mathbf{Y}(\theta)$

    -   $p(Z | \theta, \mathbf{X}) \approx q_\mathbf{X}(Z)$

------------------------------------------------------------------------

# Using the autoencoder to simulate

```{r, echo=FALSE, out.width="70%",fig.align="center"}
knitr::include_graphics("images/VarAutoencoder.png")
```

-   The optimization of the autoencoder supplies $(Z_1, \dots, Z_{N_{obs}}) = (e(x_1), \dots, e(X_{N_{obs}}))$

-   **How can we simulate the** $z's$ such that $d(z)$ looks like my original data?

-   How to construct a "machine" able to generate coherent other $Z_i$.

-   Need to constrain/ structure the latent space.

------------------------------------------------------------------------

# Probabilistic version of the autoencoder

-   **Idea** : put a prior distribution on the latent space et estimate the posterior distribution.

-   **A statistical model with latent variables**

$$X_i =d(Z_i) + \epsilon_i$$ $$Z_i \sim_{i.i.d.}N_m(0,I_m)$$ $$\epsilon_i \sim_{i.i.d.} \mathcal{N}_n(0,c I_n)$$

-   Likelihood $$\ell(\mathbf{X}; d)  =  \int_{\mathbf{Z}} p(\mathbf{X} | \mathbf{Z};d)p(\mathbf{Z})d\mathbf{Z}$$

---

# Variational Bayesian inference

- Approximate the posterior $p(\theta | Y)$ by $q(\theta)$ where $q\in \mathcal{R}$ 

<br>

- **Model** $\ell(\mathbf{Y} | \theta)$ 

- **Prior** on $\theta$ : $\pi(\theta)$

- **Approximate the posterior** $p(\theta | Y)$ by $q(\theta)$ where $q\in \mathcal{R}$  where  $\mathcal{R}$ family of simpler  distributions

    - **Example**: $q(\cdot) = \mathcal{N}(\mu,\Sigma)$

--

<br>

-  <font color="orange"> Approximating  = Minimizing the KL </font>


$$\hat{q} = \arg\min_{q \in\mathcal{R}} D_\text{KL}(q(\theta),p(\theta | \mathbf{Y}))$$
where 
$$ D_\text{KL}(q(\theta),p(\theta | \mathbf{Y})) = \mathbf{E}_q\left[\log \frac{q(\theta)}{p(\theta | \mathbf{Y})}\right]$$

---

# The magic trick






$$D_\text{KL}(q(\theta),p(\theta | \mathbf{Y}))  = \log \ell(\mathbf{Y}) - \left[\underbrace{\mathbf{E}_q[\log \ell(\mathbf{Y}|\theta)\pi(\theta)] -\mathbf{E}_q[\log q(\theta)]}_{\text{ELBO}}\right]$$ 

 

  - $\log \ell(\mathbf{Y})$ independent of $q$ 

<br>

 - Minimizing the KL is equivalent to  minimizing the Cost Function


\begin{eqnarray}
\mathcal{F}(q) &=& -  \mathbf{E}_q[\log \ell(\mathbf{Y}|\theta)\pi(\theta)] + \mathbf{E}_q[\log q(\theta)] \\
&=&  -  \mathbf{E}_q[\log \ell(\mathbf{Y}|\theta)] + \mathbf{E}_q\left[\log \frac{q(\theta)}{\pi(\theta)}\right] \\
&=&D_{\text{KL}}(q,\pi) -  \mathbf{E}_q[\log \ell(\mathbf{Y}|\theta)]   
\end{eqnarray}


---

#Parametrization of $q$ 

Choose a parametric form in $q = q_{\eta}$. 
  - **For example** : $q = \mathcal{N}(\mu,\Sigma)$ 


$$\hat{\eta} = \arg\min_{\eta} \mathcal{F}(\eta) = \arg\min_{\eta} D_{\text{KL}}(q_{\eta},\pi) - \mathbf{E}_{q_{\eta}}[\log \ell(\mathbf{Y}|\theta)]$$

  - Optimisation by gradient descent
  
  -  **BUT** expectation not explicit

  
---

# Monte Carlo approximation

  - With neural networks, $\mathbf{E}_{q_{\eta}}[\log \ell(\mathbf{Y}|\theta)]$  not explicit (activation functions non linear)
  
  - Approximation for Monte Carlo :  assume that $\theta^{(m)} \sim q_{\eta}$ 
  
$$\widehat{\mathcal{F}}(\eta) = \frac{1}{M}\sum_{m=1}^M  \log q_{\eta}(\theta ^{(m)}) - \log \pi(\theta^{(m)}) -  \log \ell(\mathbf{Y}|\theta^{(m)})$$
  
  - **Problem** we lost the explicit dependence in $\eta$ through the simulations  $\theta^{(m)}$ 
  
  - **Solution** : reparametrisation $\xi\sim \mathcal{N}(0,\mathbf{I})$, and $\phi = g(\xi,\eta)$

$$\widehat{\mathcal{F}}(\eta) = \frac{1}{M}\sum_{m=1}^M   \log q_{\eta}(\phi(\xi^{(m)},\eta)) - \log \pi(\phi(\xi^{(m)},\eta)) - \log \ell(\mathbf{Y}| \phi(\xi^{(m)},\eta))$$
 - **Remarks**
 
  - $M=1$? 
  - $D_{\text{KL}}(q_{\eta},\pi)$ may be explicit (for Gaussian distributions for instance) but not used in practice
  - $\xi^{(m)}$ are resimulated each time we compute the gradients
  
---

# More details for the regression case

- $\theta$ are the parameters (weights and bias)
- Prior gaussian distribution on $\theta$ : $\theta \sim \mathcal{N}(0, \mathbb{I})$
- If regression $$Y_i = f_\theta(X_i) + \epsilon_i, \quad \epsilon \sim \mathcal{N}(0,\sigma^2)$$

- 
$$\left[\sum_{i=1}^{N_{obs}}   \frac{||Y_i - f_{ \phi(\xi^{(m)},\eta) }(X_i)||^2}{2\sigma^2}\right]$$ 
  
---

# In the case of a neural network

-   $\theta$ are the parameters (weights and bias)
-   Prior gaussian distribution on $\theta$ : $\theta \sim \mathcal{N}(0, \mathbb{I})$
-   If regression $$Y_i = f_\theta(X_i) + \epsilon_i, \quad \epsilon \sim \mathcal{N}(0,\sigma^2)$$

$$\text{ELBO}(q) = \mathbb{E}_{q}  \left[\sum_{i=1}^{N_{obs}} - \frac{||X_i - f_{\theta}(X_i)||^2}{2\sigma^2}\right]$$

------------------------------------------------------------------------

# Approximate the posterior distribution

-   Since $\ell(\mathbf{X}; d)$ is too difficult to tackle because of the form of $p(\mathbf{Z} | \mathbf{X}; d)$

-   Let's simplify that distribution $$p(\mathbf{Z}  | \mathbf{X};d) = \prod_{i=1}^{N} p(Z_i |  X_i; d)  \approx q_{\mathbf{X}}(\mathbf{Z};g,H)  = \prod_{i=1}^{N} q_{ X_i}(Z_i;g,H)$$ $$q_{X_i}(Z_i;g,h) =\mathcal{N}_m(g(X_i),H(X_i))$$

-   Replace the likelihood by the ELBO

$$
\begin{eqnarray}
 ELBO(d,g,H) &=&\ell(\mathbf{X}; d)-  KL(q(\mathbf{Z};\mathbf{X},g,H), p\mathbf{Z} |\mathbf{X};d))\\
 &=&\mathbb{E}_{q_{\mathbf{X}}(\mathbf{Z}; ,g,H)}[\log p(\mathbf{X} , \mathbf{Z};d)] + Entr(q_{\mathbf{X}}(\mathbf{Z};g,H)))\\
 &=& \mathbb{E}_{q_{\mathbf{X}}(\mathbf{Z};g,H)}[\log p(\mathbf{X} | \mathbf{Z};d)]- KL(q_{\mathbf{X}}(\mathbf{Z};g,H), p(\mathbf{Z}))\\
\end{eqnarray}
 $$

---
# Maximize  a variational lower bound
   $$(\hat{d},\hat{g},\hat{H}) = \mbox{argmin}_{d,g,H} ELBO(d,H,g)$$ 

 $$ELBO(d,g,H)  = \mathbb{E}_{q_{\mathbf{X}}(\mathbf{Z};g,H)}[\log p(\mathbf{X} | \mathbf{Z};d)]- KL(q_{\mathbf{X}}(\mathbf{Z};g,h), p(\mathbf{Z}))$$
 
 - **Reconstruction term**
 
$$\mathbb{E}_{q_{\mathbf{X}}(\mathbf{Z};g,H)}[\log p(\mathbf{X} | \mathbf{Z};d)] = \mathbb{E}_{q_{\mathbf{X}}(\mathbf{Z};g,H)}  \left[\sum_{i=1}^N - \frac{||X_i - f(Z_i)||^2}{2c}\right]$$ 


 - **Regularisation term** :  $KL$
 
- $c$ : variance parameter which balances regularisation and  reconstruction
---

\#About d

$d$ neural network function as before

------------------------------------------------------------------------

\#About g and h

-   Are called the "encoder part"

-   $g$ and $H$:

-   $H(X)$ is a covariance so

    -   it should be a square symetric matrix
    -   **Simplification** : diagonal matrix $H(X) = h'(X)h(X)$
    -   $h(X) \in \mathbb{R}^m$

-   $h(X) = h_2(h_1(X))$, $g(X) = g_2(g_1(X))$, $g_1 = h_1$

```{r, echo=FALSE, out.width="90%",fig.align="center"}
knitr::include_graphics("images/approxZ.png")
```

------------------------------------------------------------------------

\#Evaluation of the expectation

Note that the term $\mathbb{E}_{q_{\mathbf{X}}(\mathbf{Z};g,h)} \left[\sum_{i=1}^N - \frac{||X_i - f(Z_i)||^2}{2c}\right]$ can not be evaluated.

So people simulate latent variable

$$\mathbb{E}_{q_{\mathbf{X}}(\mathbf{Z};g,h)}  \left[\sum_{i=1}^N - \frac{||X_i - f(Z_i)||^2}{2c}\right] \approx  \left[\sum_{i=1}^N - \frac{||X_i - f(g(X_i) + diag(h(X_i))\xi_i)||^2}{2c}\right]$$ where $\xi_i\sim \mathcal{N}_m(0,\mathbb{I}_m)$
