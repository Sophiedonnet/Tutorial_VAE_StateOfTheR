<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>A gentle introduction to the Variational Neural Networks</title>
    <meta charset="utf-8" />
    <meta name="author" content="J. Aubert and S. Donnet for StateOfTheR" />
    <link href="slides_VAE_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="slides_VAE_files/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="slides_VAE_files/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# A gentle introduction to the Variational Neural Networks
### J. Aubert and S. Donnet for StateOfTheR
### Dec. 2021

---





# Context 

- In statistical learning, two main tasks: 
  - **Regression or classification**
  - **Reduction of dimension**
  
- Neural networks are used to construct the regression function, classifier  or encoder-decoder (**autoencoder**). 

- ** Variational versions **  are used when we do not want to optimize a parameter but a **probability distribution**
  - if one wants to structure  the latent space
  - if one wants to perform Bayesien inference

- Relies on
  - **Neural networks** : we know already
  - **Variational EM algorithm**: we know already, but anyway it is not complicated
  

---

class:  middle
# Organisation  of the talk




&lt;font   color="orange"&gt; 1. Regression and classification, reduction of dimension &lt;/font&gt;

&lt;br&gt;

&lt;font  color="orange"&gt; 2. Neural networks &lt;/font&gt;

&lt;br&gt;


&lt;font  color="orange"&gt; 3. Variational neural networks &lt;/font&gt;

&lt;br&gt;

&lt;font  color="orange"&gt; 4. A few reminder on the optimization procedure &lt;/font&gt;







 &lt;/font&gt;


---
class:  middle


&lt;font  size="6" color="orange"&gt;  1. Basics on regression/classification, reduction of dimension &lt;/font&gt;


---

# Regression or classification


&lt;br&gt; 
- Let `\((\mathbf{X},\mathbf{Y})\)` be our dataset: 
      - `\((\mathbf{X},\mathbf{Y})=(X_i, Y_i)_{i \in 1, \dots,N_{obs}}\)`
      - `\(\forall i =1,\dots,N_{obs}\)`, **Variables** `\(X_i \in \mathbb{R}^n\)`.  
      - `\(Y_i \in \mathcal{Y}\)` the variable to explain : **classification** or **regression** 

- Looking for a function `\(f\)`

  - **classifier** or **regression**  `\(f :\mathbb{R}^n \mapsto \mathcal{Y}\)`   and  

  -  such that $$Y \approx f(X) \Leftrightarrow \mbox{Loss}(Y -   f(X)) \mbox{ small } $$ 
  
      - If **regression** `\(\mbox{Loss}(Y -   f(X))  = ||Y -   f(X)) ||^2\)` 
      
      - If **classification** : Loss = cross-entropy 
---

# Regression or classification


&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="images/RegressionClassification.png" alt="Regression or classification" width="90%" /&gt;
&lt;p class="caption"&gt;Regression or classification&lt;/p&gt;
&lt;/div&gt;

---

# Reduction of dimension

**Auto-encoders** are used for the reduction of dimension of (large) datasets. 

&lt;br&gt; 
- Let `\(X\)` be our dataset: `\(\mathbf{X}=(X_i)_{i \in 1, \dots,N_{obs}}\)`

- `\(\forall i =1,\dots,N_{obs}\)`, `\(X_i \in \mathbb{R}^n\)`.  

- Looking for two functions 

  - **Encoder** `\(e :\mathbb{R}^n \mapsto \mathbb{R}^m\)`   and  

  - **Decoder** `\(d :\mathbb{R}^m \mapsto \mathbb{R}^n\)`
  
  -  such that $$X \approx d(e(X)) \Leftrightarrow ||X -   d(e(X)) ||^2 \mbox{ small } $$ 
  
  - `\(Z = e(X)\)` : **latent variable**

---

# Autoencoder 

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="images/Autoencoder.png" alt="Autoencoder scheme." width="90%" /&gt;
&lt;p class="caption"&gt;Autoencoder scheme.&lt;/p&gt;
&lt;/div&gt;

---
class:   middle

 &lt;font  size="6" color="orange"&gt;  2. Neural networks &lt;/font&gt;



---


# About `\(f\)`: neural networks

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="images/NeuralNetwork.png" alt="Neural network architecture." width="90%" /&gt;
&lt;p class="caption"&gt;Neural network architecture.&lt;/p&gt;
&lt;/div&gt;


---

# About   `\(d\)` and `\(e\)` : neural networks

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="images/Autoencoder2.png" alt="Autoencoder scheme." width="90%" /&gt;
&lt;p class="caption"&gt;Autoencoder scheme.&lt;/p&gt;
&lt;/div&gt;




---

# About   neural networks

 

**One neuron** :     `\(f_j (\mathbf{x}) = \phi (&lt;w_j, \mathbf {x}&gt; + \, b_j)\)`   where

 
  - `\(\phi\)` the activation function : non linear
  
  - The quantities `\(w_j = (w_j^1, \dots, w_j^n)\)` are the weights of
    the input variables `\((x^1, \dots, x^n)\)`

  -   `\(b_j\)` is the bias of neuron `\(j\)`.


 

**At each layer `\(\ell\)`**  of the neural network: 
 

  - Receive `\(n_{\ell-1}\)`  input variables `\(\mathbf{y}^{\ell-1} =(y^{\ell-1}_{1}, \dots,y^{\ell-1}_{n_{\ell-1}})\)`
  
  - Create `\(n_\ell\)` new variables. For variable `\(j\)` of layer `\(l\)`: 
      `$$y^{\ell}_{j} = \phi(&lt;w^\ell_j, \mathbf{y}^{\ell-1}&gt;  +  b^{\ell}_j)$$` 
 

**Unknown parameters `\(\theta\)`**  
  - `\(w^\ell_j \in \mathbb{R}^{n_\ell-1}\)`, for `\(\ell =1, \dots L\)`, for `\(j=1,\dots,n_{\ell}\)`,
  - `\(b^\ell_j \in \mathbb{R}\)`, for `\(\ell =1, \dots L\)`, for `\(j=1,\dots,n_{\ell}\)`,



---

# Model choice

**To choose**: 
    
  - The number of layers `\(L\)`
  
  - The  number of neurons in each layer: `\(n_\ell\)` : 
      - possibly `\(n_\ell &gt; n\)`
      - For autoencoder the middle layer `\(m &lt; n\)`
  
  - The activation function `\(\phi\)`
    




---

# Learning `\(f,d\)` and `\(e\)` 


&lt;br&gt;

  - **Regression  or classification**
  
  The parameters `\(\theta = (w^\ell_j,b^\ell_j)_{j = 1\dots,n_\ell, \ell = 1,\dots,L}\)` are calibrated on a dataset `\((X_i,Y_i)_{i=1, \dots , N_{obs}}\)` by minimizing the loss function
  
  `$$\widehat{\theta} = \mbox{argmin}_{\theta \in\Theta}  \sum_{i=1}^{N_{obs}}\mbox{Loss}(Y_i - f_{\theta}(X_i))$$`
    
  - **Autoencoder** 
  
   The parameters `\(\theta = (w^\ell_j,b^\ell_j)_{j = 1\dots,n_\ell, \ell = 1,\dots,L}\)` are calibrated on a dataset `\((X_i)_{i=1, \dots , N_{obs}}\)` by minimizing the loss function
  
  `$$\widehat{\theta} = \mbox{argmin}_{\theta \in\Theta}  \sum_{i=1}^{N_{obs}}||X_i - d_{\theta}\circ e_{\theta}(X_i)||^2$$` 


**Optimisation by Stochastic gradient descent**: see later for a reminder of the principle


---

# PCA versus autoencoder


- Let `\(W \in M_{n,m}(\mathbb{R})\)`, `\(W = (C_1,\dots,C_m)\)` 

- The  `\(C_i\)`'s are `\(m\)` columns vectors, `\(m &lt; n\)`. 

- **Hyp.**:  `\((C_i)\)` are orthonormal vectors. Consequently:
  `$$W'W = I_n$$`
  
  
- Let `\(W' X_i\)` is the projector of vector `\(X_i\)` on the sub-vectorial space generated by the columns of `\(W\)`. 
  
-  We are looking for  `\(W\)` minimizing the inertia of the projected dataset: 
$$
`\begin{aligned}
W^* &amp;=\mbox{argmax}_{\{W \in M_{n,m}(\mathbb{R}), W'W = I_n\}} \sum_{i=1}^{N_{obs}} || W'X_i||^2\\ &amp;=\mbox{argmin}_{\{W \in M_{n,m}(\mathbb{R}), W'W = I_n\}} \sum_{i=1}^{N_{obs}} || X_i - WW'X_i||^2
\end{aligned}`
$$

---

# PCA versus autoencoder

- `\(W' = e\)`  **linear** encoder function

- `\(W = d\)` : **linear** decoder function

- Note that if you use neural networks with linear activation function and one layer, you will get `\(W\)` not necessarily orthogonal. 


[Lien vers une démonstration propre](http://www.xavierdupre.fr/app/mlstatpy/helpsphinx/c_ml/rn/rn_9_auto.html)

---
class: middle

&lt;font  size="6" color="orange"&gt; 3. Variational version &lt;/font&gt;

 


---

# Why  variational neural networks?

  -  **Regression-Classification** : Bayesian inference of the parameters `\(\theta\)`
      
      - Prior on `\(\theta\)`: `\(\pi(\theta)\)`
      
      - Estimation not of `\(\theta\)` but of the posterior distribution of `\(\theta\)` : `\(p(\theta | \mathbf{Y})\)`
      
  - **Autoencoder**: give a structure on the latent space `\(\mathbf{Z}\)`
      
      - Prior on `\(Z\)`: `\(\pi(Z)\)`
      
      - Estimation of `\(\theta\)` and of  the posterior distribution of `\(Z\)` : `\(p(Z | \theta,  \mathbf{X})\)`

  - **Variational** : approximation of the distributions 
      
      -  `\(p(\theta | \mathbf{Y}) \approx q_\mathbf{Y}(\theta)\)`
      
      -  `\(p(Z | \theta,  \mathbf{X}) \approx q_\mathbf{X}(Z)\)`
          
  

---

# Using the autoencoder to simulate  



&lt;img src="images/VarAutoencoder.png" width="70%" style="display: block; margin: auto;" /&gt;


  -   The optimization of the autoencoder supplies `\((Z_1, \dots, Z_{N_{obs}}) = (e(x_1), \dots, e(X_{N_{obs}}))\)`
  
  - **How can we simulate the `\(z's\)` such that `\(d(z)\)` looks like my original data?**  

  -  How to construct a "machine" able to generate coherent other `\(Z_i\)`.   
  
  - Need to  constrain/ structure the latent space. 
  
---

# Probabilistic version of the autoencoder
  
  -  **Idea** : put a prior distribution on the latent space et estimate the posterior distribution.  


  - **A statistical model with latent variables**
  
  `$$X_i =d(Z_i) + \epsilon_i$$`
  `$$Z_i \sim_{i.i.d.}N_m(0,I_m)$$`
  `$$\epsilon_i \sim_{i.i.d.} \mathcal{N}_n(0,c I_n)$$`
 
  
  - Likelihood 
   `$$\ell(\mathbf{X}; d)  =  \int_{\mathbf{Z}} p(\mathbf{X} | \mathbf{Z};d)p(\mathbf{Z})d\mathbf{Z}$$`
  
  **Not explicit**
  
  - EM requires the posterior distribution of `\(\mathbf{Z}\)`

  $$p(\mathbf{Z} | \mathbf{X}; d) \propto p(\mathbf{X}|\mathbf{Z}; d)p(\mathbf{Z}) $$
  **Very complex  too**



---
class: middle

&lt;font  size="6" color="orange"&gt; 3.1.  Bayesian inference for neural networks &lt;/font&gt;




---

# Variational Bayesian inference


&lt;br&gt;

- **Model** `\(\ell(\mathbf{Y} | \theta)\)` 

- **Prior** on `\(\theta\)` : `\(\pi(\theta)\)`

- **Approximate the posterior** `\(p(\theta | Y)\)` by `\(q(\theta)\)` where `\(q\in \mathcal{R}\)`  where  `\(\mathcal{R}\)` family of simpler  distributions

    - **Example**: `\(q(\cdot) = \mathcal{N}(\mu,\Sigma)\)`

--

&lt;br&gt;

-  &lt;font color="orange"&gt; Approximating  = Minimizing the KL &lt;/font&gt;


`$$\hat{q} = \arg\min_{q \in\mathcal{R}} D_\text{KL}(q(\theta),p(\theta | \mathbf{Y}))$$`
where 
$$ D_\text{KL}(q(\theta),p(\theta | \mathbf{Y})) = \mathbf{E}_q\left[\log \frac{q(\theta)}{p(\theta | \mathbf{Y})}\right]$$

---

# The magic trick






`$$D_\text{KL}(q(\theta),p(\theta | \mathbf{Y}))  = \log \ell(\mathbf{Y}) - \left[\underbrace{\mathbf{E}_q[\log \ell(\mathbf{Y}|\theta)\pi(\theta)] -\mathbf{E}_q[\log q(\theta)]}_{\text{ELBO}}\right]$$` 

 

  - `\(\log \ell(\mathbf{Y})\)` independent of `\(q\)` 

&lt;br&gt;

 - Minimizing the KL is equivalent to  minimizing the Cost Function


`\begin{eqnarray}
\mathcal{F}(q) &amp;=&amp; -  \mathbf{E}_q[\log \ell(\mathbf{Y}|\theta)\pi(\theta)] + \mathbf{E}_q[\log q(\theta)] \\
&amp;=&amp;  -  \mathbf{E}_q[\log \ell(\mathbf{Y}|\theta)] + \mathbf{E}_q\left[\log \frac{q(\theta)}{\pi(\theta)}\right] \\
&amp;=&amp;D_{\text{KL}}(q,\pi) -  \mathbf{E}_q[\log \ell(\mathbf{Y}|\theta)]   
\end{eqnarray}`


---

#Parametrization of `\(q\)` 

Choose a parametric form in `\(q = q_{\eta}\)`. 
  - **For example** : `\(q = \mathcal{N}(\mu,\Sigma)\)` 


`$$\hat{\eta} = \arg\min_{\eta} \mathcal{F}(\eta) = \arg\min_{\eta} D_{\text{KL}}(q_{\eta},\pi) - \mathbf{E}_{q_{\eta}}[\log \ell(\mathbf{Y}|\theta)]$$`

  - Optimisation by gradient descent
  
  -  **BUT** expectation not explicit

  
---

# Monte Carlo approximation

  - With neural networks, `\(\mathbf{E}_{q_{\eta}}[\log \ell(\mathbf{Y}|\theta)]\)`  not explicit (activation functions non linear)
  
  - Approximation for Monte Carlo :  assume that `\(\theta^{(m)} \sim q_{\eta}\)` 
  
`$$\widehat{\mathcal{F}}(\eta) = \frac{1}{M}\sum_{m=1}^M  \log q_{\eta}(\theta ^{(m)}) - \log \pi(\theta^{(m)}) -  \log \ell(\mathbf{Y}|\theta^{(m)})$$`
  
  - **Problem** we lost the explicit dependence in `\(\eta\)` through the simulations  `\(\theta^{(m)}\)` 
  
  - **Solution** : reparametrisation `\(\xi\sim \mathcal{N}(0,\mathbf{I})\)`, and `\(\phi = g(\xi,\eta)\)`

`$$\widehat{\mathcal{F}}(\eta) = \frac{1}{M}\sum_{m=1}^M   \log q_{\eta}(\phi(\xi^{(m)},\eta)) - \log \pi(\phi(\xi^{(m)},\eta)) - \log \ell(\mathbf{Y}| \phi(\xi^{(m)},\eta))$$`
 - **Remarks**
 
  - `\(M=1\)`? 
  - `\(D_{\text{KL}}(q_{\eta},\pi)\)` may be explicit (for Gaussian distributions for instance) but not used in practice
  - `\(\xi^{(m)}\)` are resimulated each time we compute the gradients
  
---

# More details for the regression case

- `\(\theta\)` are the parameters (weights and bias)
- Prior gaussian distribution on `\(\theta\)` : `\(\theta \sim \mathcal{N}(0, \mathbb{I})\)`
- If regression `$$Y_i = f_\theta(X_i) + \epsilon_i, \quad \epsilon \sim \mathcal{N}(0,\sigma^2)$$`

- 
`$$\log \ell(\mathbf{Y}| \phi(\xi^{(m)},\eta)) =   \left[\sum_{i=1}^{N_{obs}} - \frac{||Y_i - f_{ \phi(\xi^{(m)},\eta) }(X_i)||^2}{2\sigma^2}\right]$$` 
  
---

# Approximate the posterior distribution


 - Since  `\(\ell(\mathbf{X}; d)\)`  is too difficult to tackle because of the form of `\(p(\mathbf{Z} | \mathbf{X}; d)\)`

 - Let's   simplify that distribution 
  `$$p(\mathbf{Z}  | \mathbf{X};d) = \prod_{i=1}^{N} p(Z_i |  X_i; d)  \approx q_{\mathbf{X}}(\mathbf{Z};g,H)  = \prod_{i=1}^{N} q_{ X_i}(Z_i;g,H)$$`
  `$$q_{X_i}(Z_i;g,h) =\mathcal{N}_m(g(X_i),H(X_i))$$`


- Replace the likelihood by the ELBO 

 $$
`\begin{eqnarray}
 ELBO(d,g,H) &amp;=&amp;\ell(\mathbf{X}; d)-  KL(q(\mathbf{Z};\mathbf{X},g,H), p\mathbf{Z} |\mathbf{X};d))\\
 &amp;=&amp;\mathbb{E}_{q_{\mathbf{X}}(\mathbf{Z}; ,g,H)}[\log p(\mathbf{X} , \mathbf{Z};d)] + Entr(q_{\mathbf{X}}(\mathbf{Z};g,H)))\\
 &amp;=&amp; \mathbb{E}_{q_{\mathbf{X}}(\mathbf{Z};g,H)}[\log p(\mathbf{X} | \mathbf{Z};d)]- KL(q_{\mathbf{X}}(\mathbf{Z};g,H), p(\mathbf{Z}))\\
\end{eqnarray}`
 $$
 
 
 
---
# Maximize  a variational lower bound
   `$$(\hat{d},\hat{g},\hat{H}) = \mbox{argmin}_{d,g,H} ELBO(d,H,g)$$` 

 `$$ELBO(d,g,H)  = \mathbb{E}_{q_{\mathbf{X}}(\mathbf{Z};g,H)}[\log p(\mathbf{X} | \mathbf{Z};d)]- KL(q_{\mathbf{X}}(\mathbf{Z};g,h), p(\mathbf{Z}))$$`
 
 - **Reconstruction term**
 
`$$\mathbb{E}_{q_{\mathbf{X}}(\mathbf{Z};g,H)}[\log p(\mathbf{X} | \mathbf{Z};d)] = \mathbb{E}_{q_{\mathbf{X}}(\mathbf{Z};g,H)}  \left[\sum_{i=1}^N - \frac{||X_i - f(Z_i)||^2}{2c}\right]$$` 


 - **Regularisation term** :  `\(KL\)`
 
- `\(c\)` : variance parameter which balances regularisation and  reconstruction


---

#About d

`\(d\)` neural network function as before

---

#About g and h

 - Are called the "encoder part"
 - `\(g\)` and `\(H\)`: 
 - `\(H(X)\)` is a covariance so 
    - it should be a square symetric matrix 
    - **Simplification** : diagonal matrix `\(H(X) = h'(X)h(X)\)`
    - `\(h(X) \in \mathbb{R}^m\)`
  - `\(h(X) = h_2(h_1(X))\)`, `\(g(X) = g_2(g_1(X))\)`, `\(g_1 = h_1\)`
  
  
&lt;img src="images/approxZ.png" width="90%" style="display: block; margin: auto;" /&gt;


    
---
#Evaluation of the expectation

Note that the term 
    `\(\mathbb{E}_{q_{\mathbf{X}}(\mathbf{Z};g,h)}  \left[\sum_{i=1}^N - \frac{||X_i - f(Z_i)||^2}{2c}\right]\)`
can not be evaluated. 

So people simulate latent variable

`$$\mathbb{E}_{q_{\mathbf{X}}(\mathbf{Z};g,h)}  \left[\sum_{i=1}^N - \frac{||X_i - f(Z_i)||^2}{2c}\right] \approx  \left[\sum_{i=1}^N - \frac{||X_i - f(g(X_i) + diag(h(X_i))\xi_i)||^2}{2c}\right]$$`
where `\(\xi_i\sim \mathcal{N}_m(0,\mathbb{I}_m)\)`
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
